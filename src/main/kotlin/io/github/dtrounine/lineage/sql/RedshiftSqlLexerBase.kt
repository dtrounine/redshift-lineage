package io.github.dtrounine.lineage.sql

import io.github.dtrounine.lineage.sql.parser.generated.RedshiftSqlLexer
import org.antlr.v4.kotlinruntime.CharStream
import org.antlr.v4.kotlinruntime.Lexer
import java.util.*

abstract class RedshiftSqlLexerBase(inputStream: CharStream): Lexer(inputStream) {
    // This is a placeholder for the actual lexer implementation
    // The actual lexer would be generated by ANTLR based on the grammar file
    // and would contain the logic for tokenizing the SQL input.

    protected val tags: Stack<String> = Stack()

    fun PushTag() {
        tags.push(text)
    }

    fun IsTag(): Boolean {
        return text == tags.peek()
    }

    fun PopTag() {
        tags.pop()
    }

    fun UnterminatedBlockCommentDebugAssert() {
        //Debug.Assert(InputStream.LA(1) == -1 /*EOF*/);
    }

    fun CheckLaMinus(): Boolean {
        return inputStream.LA(1).toChar() != '-'
    }

    fun CheckLaStar(): Boolean {
        return inputStream.LA(1).toChar() != '*'
    }

    fun CharIsLetter(): Boolean {
        return Character.isLetter(inputStream.LA(-1))
    }

    fun HandleNumericFail() {
        inputStream.seek(inputStream.index() - 2)
        type = RedshiftSqlLexer.Tokens.Integral
    }

    fun HandleLessLessGreaterGreater() {
        if (text == "<<") type = RedshiftSqlLexer.Tokens.LESS_LESS
        if (text == ">>") type = RedshiftSqlLexer.Tokens.GREATER_GREATER
    }

    fun CheckIfUtf32Letter(): Boolean {
        var codePoint: Int = inputStream.LA(-2) shl 8 + inputStream.LA(-1)
        val c: CharArray
        if (codePoint < 0x10000) {
            c = charArrayOf(codePoint.toChar())
        } else {
            codePoint -= 0x10000
            c = charArrayOf((codePoint / 0x400 + 0xd800).toChar(), (codePoint % 0x400 + 0xdc00).toChar())
        }
        return Character.isLetter(c[0])
    }

    fun IsSemiColon(): Boolean {
        return ';' == inputStream.LA(1).toChar()
    }


}